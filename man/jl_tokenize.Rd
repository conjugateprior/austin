% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/jl.R
\name{jl_tokenize}
\alias{jl_tokenize}
\title{Split each text into tokens}
\usage{
jl_tokenize(x, tokenizer = tokenizers::tokenize_words, ...)
}
\arguments{
\item{x}{a tibble}

\item{tokenizer}{a function that returns a list of character vectors containing tokens}

\item{...}{extra arguments to 'tokenizer'}
}
\value{
jl_df
}
\description{
Fills the 'tokens' variable with tokens. Tokens can be any strings, although 
in the usual cases are words or category/topic labels from a content 
analysis dictionary.
}
\details{
Examples:

To tokenize words, leave the tokenizer function at its default and add
and extra arguments, e.g. strip_numeric = TRUE, in the ...

To tokenize into category/topic labels using quanteda's resources 
set the tokenizer function to (here 
assuming you have the quanteda library loaded - if not, preface every function 
name with quanteda::)

dict_tok <- function(txt){ 
  tokens_lookup(tokens(txt, remove_punct = TRUE),
                       dictionary = mydictionary)
}

and run as jl_tokenize(mydf, tokenizer = dict_tok)

This function adds a list column called 'tokens' to contain the tokens
}
