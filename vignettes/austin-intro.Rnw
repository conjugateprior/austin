\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[bitstream-charter]{mathdesign}
\usepackage{inconsolata}


\usepackage[margin=1.2in]{geometry}
\usepackage{microtype}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{color}
\definecolor{darkblue}{rgb}{0,0,0.5}
\usepackage[colorlinks=true,linkcolor=darkblue,citecolor=darkblue,urlcolor=darkblue]{hyperref}
\usepackage[noae,nogin]{Sweave}

\title{An Introduction to Austin}
\author{Will Lowe\\MZES\\University of Mannheim}

%\VignetteIndexEntry{An R package for doing things with words} 
%\VignettePackage{austin}

\begin{document}
\SweaveSyntax{SweaveSyntaxLatex}

\maketitle

\section{Getting Data In}

Austin works with any two dimensional matrix-like object for which \texttt{is.wfm} returns TRUE.  
An object is a wfm when it is indexable in two dimensions, has a complete set of row and column names,
and has the dimension names `docs' and `words'.  Whether these are on rows or columns 
does not matter.  The function \texttt{wfm} will construct a suitable object
from any column and row labeled matrix-like object such as a data.frame or matrix.

Austin offers the helper functions
\texttt{as.worddoc} and \texttt{as.docword} to extract the raw count
data the appropriate way up.  That is, as a matrix where words are
rows and documents are columns, or where documents are rows and words
are columns.  The \texttt{docs} and \texttt{words} return vectors of
document names and words respectively.  \texttt{getdocs} is used for
extracting particular sets of documents from a wfm by name or index.

The function 
\texttt{trim} can be used to remove low frequency words and words that occur in
only a few documents.  It can also be used to sample randomly from the set of words.  
This can be helpful to speed up analyses and check robustness to vocabulary choice.

\subsection{Importing CSV Data}

If you already have word frequency data in a file in comma-separated value
(.csv) format you can read it in using
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> data <- wfm('mydata.csv')
\end{Verbatim} 
This function assumes that the word labels are in the first column and 
document names are in the first row, pretty much as
\href{http://www.williamlowe.net/software/}{JFreq} would offer it to
you by default.  Assuming that words are rows and documents are columns
use.  If words are columns, add a \texttt{word.margin=2} argument.  

\subsection{Counting Words from Inside R}

Assuming Java is installed on your system then you can count words in
text files and generate an appropriate \texttt{wfm} object in one step
using the \texttt{jwordfreq} function\footnote{Although you'll
  probably have more control over the process using JFreq}.

\section{Scaling with Wordfish}

Austin implements the one dimensional text scaling model Wordfish
\citep{SlapinProksch2008,ProkschSlapin2008}.  When document positions
are random variables the model is known as Rhetorical Ideal Points
\citep{MonroeMaeda04} which is formally equivalent to a Item Response
Theory and closely related to the generalized Latent Trait models with a Poisson link
\citep[e.g.][]{MoustakiKnott2000}.  

Austin
implements a version of Wordfish with faster convergence, analytic
or bootstrapped standard errors, and integration into R's usual model
functions, summary, coef, predict, etc. .

This model class has two equivalent parameterizations.  Assume $D$
documents, $V$ words, and a $D\times V$ matrix of word counts $Y$.
Represent the $i$th row as $y_i=[Y_{i1}\ldots Y_{iV}]$ which contains
$N_i=\sum y_i$ words in total.  In the first parameterization
\begin{align*}
P(y_i \mid \theta) &= \prod^D_{j=1} P(Y_{ij} \mid \theta_i)\\
P(Y_{ij} \mid \theta_i) &= \text{Poisson}(\mu_{ij})\\
\mu_{ij} &= \psi_j + \beta_j\theta_i + \alpha_i.
\end{align*}
That is, word counts are conditionally independent given
knowledge of the text's position, and conditional on position word
counts are independent Poisson processes depending on word-specific
intercept and slope parameters and document-specific position and offset
parameters.

In the Austin implementation the
parameters are estimated by a Conditional Maximum Likelihood with a
regularization constraint on $\beta$s that is interpretable as a
shared zero mean prior with standard deviation $\sigma$.  

If we assume that the length of a document $N_i$ is uninformative
about $\theta_i$, we may condition on it.  This leads to second
Multinomial parameterization of the model
\begin{align*}
P(y_i \mid \theta) &= \text{Multinomial}(\pi_{ij}, N_i)\\
\log \frac{\pi_j}{\pi_1} &= \psi_j^* + \beta_j^* \theta_i 
\end{align*}
where the starred parameters are linear transformations of their
namesakes in the first parameterization and the document specific
offsets cancel out \citep{LoweWF}.  This is the form of
the model reported by Austin and used for prediction.  
Austin treats
the first parameterization as a computational convenience to make
estimation more efficient.  The \texttt{coef} function takes a \texttt{form}
parameter if you need to see it.

\begin{Scode}{echo=FALSE,fig=FALSE}
  options(width=100)
\end{Scode}

We start by loading the package
\begin{Scode}{echo=TRUE,fig=FALSE}
  library('austin')
\end{Scode}
and generating an (unrealistically small) set of test data 
according to the assumptions above
\begin{Scode}{echo=TRUE,fig=FALSE}
dd <- sim.wordfish(docs=10, vocab=12)
\end{Scode}

The resulting object is of class \texttt{sim.wordfish} and contains
the generating parameters (in the form of the first model).  The two elements
of interest are the vector of document positions
\begin{Scode}{echo=TRUE,fig=FALSE}
dd$theta
\end{Scode}
and the generated data Y
\begin{Scode}{echo=TRUE,fig=FALSE}
as.worddoc(dd$Y)
\end{Scode}
where Y is an object of class \texttt{wfm}.

To scale this data we use the wordfish function
\begin{Scode}{echo=TRUE,fig=FALSE}
wf <- wordfish(dd$Y)
\end{Scode}
The model is by default globally identified by requiring that
$\hat{\theta}_{10} > \hat{\theta}_1$.  This will
be true for all simulated data (with more than 10 documents).  For real
data more suitable values may be set using the \texttt{dir} argument.

Estimated document positions can be summarized using
\begin{Scode}{echo=TRUE,fig=FALSE}
summary(wf)
\end{Scode}
To examine the word-specific parameters use
\begin{Scode}{echo=TRUE,fig=FALSE}
coef(wf)
\end{Scode}

Estimated document positions and 95\% confidence intervals can also be
graphed\footnote{For more than a few tens of words the confidence
  intervals will probably be `implausibly' small.  They are
  nevertheless asymptotically correct given the model assumptions. It
  is those assumptions you might doubt.}.  Any unnamed second argument
to the plot function is taken as a vector of true document positions.
These are then plotted over the original plot, as shown in Figure~\ref{fig1}.

\begin{Scode}{simwf,echo=FALSE,fig=TRUE,include=FALSE}
plot(wf, dd$theta)
\end{Scode}

\begin{figure}[tbp]
\centerline{ \includegraphics[scale=0.7]{austin-intro-simwf} }
\caption{Wordfish position estimates on simulated data using the
\texttt{plot(wf, dd\$theta)}.}
\label{fig1}
\end{figure}

Positions for new documents can also be estimated.  Here we generate
predictions and confidence intervals for existing documents D4 and D5
in the original data set
\begin{Scode}{echo=TRUE,fig=FALSE}
predict(wf, newdata=getdocs(dd$Y, c(4,5)), se.fit=TRUE, interval='conf')
\end{Scode}

\section{Scaling with Wordscores}

Wordscores \citep{Laveretal2003} is a method for scaling texts closely
related to both correspondence analysis by implementing an incomplete
reciprocal averaging algorithm, and to quadratic ordination as an
approximation to an unfolding model \citep{Lowe2008}.

Austin refers to the algorithm described in \citep{Laveretal2003} as
`classic' Wordscores to distinguish it from versions closer to
correspondence analysis.  A classic Wordscores analysis has several
distinguishing features.

\subsection{Classic Wordscores}

Classic Wordscores estimates scores for words (`wordscores') using
only word frequency information from documents with known positions
(`reference' documents).  There is therefore no iterative estimation
process since document positions are observed.  Documents with unknown
positions (`virgin' documents) are treated as out of sample.

Positions for out of sample documents are estimated by averaging the scores of
the words they contain and re-scaling in an ad-hoc fashion that has
generated some discussion and various alternatives.  The method also offers
standard errors for the out of sample documents\footnote{These are
  probably incorrect -- partly because the probability model from they
  would have to be derived is unclear and partly because they can be 
  quite implausible in some applications.}

To replicate the example analysis in Laver et al. (2003) we begin loading
the test data
\begin{Scode}{echo=TRUE,fig=FALSE}
data(lbg)
\end{Scode}
So we take a look at the word counts we've got to work with
\begin{Scode}{echo=TRUE,fig=FALSE}
as.docword(lbg)
\end{Scode}
and then fit a classic Wordscores model to them.  Assume we know the
positions of document R1 through R5 and wish to scale V1.  

We first separate the reference documents from the virgin document:
\begin{Scode}{echo=TRUE,fig=FALSE}
ref <- getdocs(lbg, 1:5)
vir <- getdocs(lbg, 'V1') 
\end{Scode}
then fit the model using the reference documents
\begin{Scode}{echo=TRUE,fig=FALSE}
ws <- classic.wordscores(ref, scores=seq(-1.5,1.5,by=0.75))
\end{Scode}

We can summarise the results
\begin{Scode}{echo=TRUE,fig=FALSE}
summary(ws)
\end{Scode}
The summary presents details about the reference documents.  If we want to see the 
wordscores that were generated we look for the model's coefficients
\begin{Scode}{echo=TRUE,fig=FALSE}
coef(ws)
\end{Scode}
which can also be plotted.

We can now get a position for the virgin document
\begin{Scode}{echo=TRUE,fig=FALSE}
predict(ws, newdata=vir)
\end{Scode}
When more than one document is to be predicted, an ad-hoc procedure 
is applied by default to the predicted positions to rescale them to the same
variance as the reference scores.  This may or may not be what you want.

\subsection{Correspondence Analysis}

Wordscores approximates correspondence analysis, which is defined for
more than one dimension.  To explore this approach to document scaling
you may find the \texttt{corresp} function from the \texttt{MASS}
package useful.  In addition, Mair and de Leeuw's \texttt{anacor}
package provides a very nice package devoted to these methods.


\newpage
\bibliographystyle{apsr}

\bibliography{austin}

\end{document}
